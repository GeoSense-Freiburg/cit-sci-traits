{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1.4: Build GBIF trait maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before training models using Earth observation (EO) data is to link the TRY trait data with the GBIF species observations and then grid them. In this way, we can have matching trait rasters to be paired with our EO data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.config as dask_config\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as riox\n",
    "from src.conf.conf import get_config\n",
    "from src.conf.environment import log\n",
    "from src.utils.raster_utils import create_sample_raster, xr_to_raster\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Display all columns when printing a pandas DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "cfg = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the GBIF data, we're going to need to use Dask in order to keep memory usage low as well as to parallelize the merging and spatial gridding operations. The settings below (`n_workers` and `memory_limit`, in particular) are specific to the machine being used during this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=50, memory_limit=\"24GB\", dashboard_address=\":39143\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GBIF and filter by PFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the GBIF data, select all three PFTs, and set species name as the index to make merging with the TRY data faster.\n",
    "\n",
    "**Note:** Processing the entire GBIF dataset, as done below, may be infeasible for some machines. If this is the case, simply select a single PFT for the `filter_pft` call, and also consider using `DataFrame.sample(frac=<fraction of the data>)` to use only a subsample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pft(df: pd.DataFrame, pft_set: str, pft_col: str = \"pft\") -> pd.DataFrame:\n",
    "    pfts = pft_set.split(\"_\")\n",
    "    if not any(pft in [\"Shrub\", \"Tree\", \"Grass\"] for pft in pfts):\n",
    "        raise ValueError(f\"Invalid PFT designation: {pft_set}\")\n",
    "\n",
    "    return df[df[pft_col].isin(pfts)]\n",
    "\n",
    "npartitions = 90\n",
    "\n",
    "gbif = (\n",
    "    dd.read_parquet(Path(cfg.gbif.interim.dir, cfg.gbif.interim.subsampled))\n",
    "    .repartition(npartitions=npartitions)\n",
    "    .pipe(filter_pft, \"Shrub_Tree_Grass\")\n",
    "    .set_index(\"speciesname\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TRY filtered mean trait data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_traits = (\n",
    "    dd.read_parquet(Path(cfg.trydb.interim.dir, cfg.trydb.interim.filtered))\n",
    "    .repartition(npartitions=npartitions)\n",
    "    .set_index(\"speciesname\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link mean trait values with GBIF data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set species name as the index on both DataFrames, we can simply perform an inner join, called by the GBIF data, to merge the traits and cit-sci species occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = gbif.join(mn_traits, how=\"inner\").reset_index().drop(columns=[\"pft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of unique indices\n",
    "print(\n",
    "    f\"Pct matched species: {merged.index.nunique().compute() / gbif.index.nunique():.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasterize merged trait values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid the matched trait data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_grid_df(\n",
    "    df: dd.DataFrame,\n",
    "    col: str,\n",
    "    lon: str = \"decimallongitude\",\n",
    "    lat: str = \"decimallatitude\",\n",
    "    res: int | float = 0.5,\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group and aggregate a DataFrame by latitude and longitude coordinates to create a\n",
    "    gridded DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (dd.DataFrame): The input DataFrame.\n",
    "        col (str): The column to aggregate.\n",
    "        lon (str, optional): The column name for longitude coordinates. Defaults to\n",
    "            \"decimallongitude\".\n",
    "        lat (str, optional): The column name for latitude coordinates. Defaults to\n",
    "            \"decimallatitude\".\n",
    "        res (int | float, optional): The resolution of the grid. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        dd.DataFrame: The gridded DataFrame with aggregated statistics.\n",
    "\n",
    "    \"\"\"\n",
    "    stat_funcs = [\n",
    "        \"mean\",\n",
    "        \"std\",\n",
    "        \"median\",\n",
    "        lambda x: x.quantile(0.05, interpolation=\"nearest\"),\n",
    "        lambda x: x.quantile(0.95, interpolation=\"nearest\"),\n",
    "    ]\n",
    "    \n",
    "    stat_names = [\"mean\", \"std\", \"median\", \"q05\", \"q95\"]\n",
    "\n",
    "    # Calculate the bin for each row directly. This may be very slightly less accurate\n",
    "    # than creating x and y bins and using `pd.cut`, but it has the benefit of being\n",
    "    # significantly more performant.\n",
    "    df[\"y\"] = (df[lat] + 90) // res * res - 90 + res / 2\n",
    "    df[\"x\"] = (df[lon] + 180) // res * res - 180 + res / 2\n",
    "\n",
    "    gridded_df = (\n",
    "        df.drop(columns=[lat, lon])\n",
    "        .groupby([\"y\", \"x\"], observed=False)[[col]]\n",
    "        .agg(stat_funcs)\n",
    "    )\n",
    "\n",
    "    gridded_df.columns = stat_names\n",
    "\n",
    "    return gridded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_df_to_raster(df: pd.DataFrame, res: int | float, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts a grid DataFrame to a raster file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The grid DataFrame to convert.\n",
    "        res (int | float): The resolution of the raster.\n",
    "        name (str): The name of the raster file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    rast = create_sample_raster(resolution=res)\n",
    "    ds = df.to_xarray()\n",
    "    ds = ds.rio.write_crs(rast.rio.crs)\n",
    "    ds = ds.rio.reproject_match(rast)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        nodata = ds[var].attrs[\"_FillValue\"]\n",
    "        ds[var] = ds[var].where(ds[var] != nodata, np.nan)\n",
    "        ds[var] = ds[var].rio.write_nodata(-32767.0, encoded=True)\n",
    "    \n",
    "    ds.attrs[\"long_name\"] = list(ds.data_vars)\n",
    "    ds.attrs[\"trait\"] = name\n",
    "\n",
    "    xr_to_raster(ds, f\"{name}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grid the data for the first trait, \"X4\" or \"stem specific density\". \n",
    "\n",
    "`global_grid_df` grids the data to the centroids of each point observations corresponding grid cell (at the desired resolution), and then calculates the mean, standard deviation, median, and 5th and 95th quantiles of each grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 15:12:15,777 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle de3bbcc45610cb7e4ac5c7fbc59e0bbb initialized by task ('shuffle-transfer-de3bbcc45610cb7e4ac5c7fbc59e0bbb', 53) executed on worker tcp://127.0.0.1:32879\n",
      "2024-05-09 15:12:21,958 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d731f27fed7ecff24e128e8182c0c9bc initialized by task ('shuffle-transfer-d731f27fed7ecff24e128e8182c0c9bc', 0) executed on worker tcp://127.0.0.1:32879\n",
      "2024-05-09 15:12:24,110 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle de3bbcc45610cb7e4ac5c7fbc59e0bbb deactivated due to stimulus 'task-finished-1715267544.0555131'\n",
      "2024-05-09 15:12:34,841 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d731f27fed7ecff24e128e8182c0c9bc deactivated due to stimulus 'task-finished-1715267554.8393872'\n"
     ]
    }
   ],
   "source": [
    "cols = [col for col in merged.columns if col.startswith(\"X\")]\n",
    "grid_data = global_grid_df(merged, cols[0], res=0.01).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's fill a raster with the gridded data and save it to file.\n",
    "\n",
    "Note that in `grid_df_to_raster` we first generate a reference raster at the desired resolution and then match the `xarray.Dataset` we created from our gridded DataFrame to the reference raster. This is important, because, due to minor differences in floating point accuracy, the rasterized DataFrame's coordinates may be subtly different than those of our EO predictor data. If we first match all training data to a reference raster, however, we can ensure that all coordinates will match perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df_to_raster(grid_data, 0.01, \"X4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, let's shut down our Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
