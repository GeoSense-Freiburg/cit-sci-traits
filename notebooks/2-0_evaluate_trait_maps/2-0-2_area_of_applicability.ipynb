{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0.2: Area of Applicability (AoA)\n",
    "\n",
    "While spatial K-fold cross-validation (SKCV) is useful for minimizing spatial dependence on model error estimation, it may not be sufficient for explaining the transferability of model extrapolations in areas where predictor data varies significantly from the reference data (Meyer & Pebesma, 2021). To address this, we can apply methods developed by Meyer & Pebesma (2021) for determining the dissimilarity index (DI) between unseen predictor data (“new”) and predictor data on which models were trained (“train”), and thus the area of applicability (AoA) of the final trait maps.\n",
    "\n",
    "Briefly, dissimilarity in the predictor space is calculated by first computing the average minimum distance between observations in each cross-validation fold in “train” (i.e. the minimum distances between points in one fold from points in all other folds), followed by calculating the minimum distances between observations in the “new” data from the training data. The DI can then be determined as “new” distances divided by mean “train” distance. The 95% percentile DI value can then be set as the threshold value for determining the AoA, with values below the threshold being within the AoA and values above the threshold being outside it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from src.utils.dataset_utils import get_predict_imputed_fn, get_y_fn\n",
    "from src.utils.training_utils import assign_splits, filter_trait_set, set_yx_index\n",
    "from src.conf.conf import get_config\n",
    "from src.conf.environment import log\n",
    "\n",
    "cfg = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Determine average minimum distance in training feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of calculating the dissimilarity index (DI) is to calculate the minimum distances between all observations in the feature space of the training data, and to then determine the average minimum distance from that. This provides a sort of normalization coefficient that will be used to determine how similar observations in the predict feature space are to those in train.\n",
    "\n",
    "However, to avoid comparing observations (or points) to other points within their spatial autocorrelation range, we need to ensure that distances are calculated only to points outside of each point's respective spatial cross-validation fold.\n",
    "\n",
    "Additionally, because not all features are as influential as the others, we will also need to first normalize the feature data and then weight them according to their respective importance as determined during model training with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the training data for the combined sPlot and GBIF trait dataset along with their fold assignments (AKA CV splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_COL: str = \"X11_mean\"\n",
    "\n",
    "splot_gbif = (\n",
    "    pd.read_parquet(get_y_fn(), columns=[\"x\", \"y\", Y_COL, \"source\"])\n",
    "    .pipe(set_yx_index)\n",
    "    .pipe(assign_splits, label_col=Y_COL)\n",
    "    .pipe(filter_trait_set, trait_set=\"splot_gbif\")\n",
    "    .drop(columns=[Y_COL, \"source\"])\n",
    "    .merge(\n",
    "        # Merge using inner join with the imputed predict data (described below)\n",
    "        dd.read_parquet(get_predict_imputed_fn()).compute().pipe(set_yx_index),\n",
    "        how=\"inner\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        validate=\"1:1\"\n",
    "    )\n",
    "    .reset_index(drop=True) # We no longer need the x and y indices\n",
    ")\n",
    "splot_gbif.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize features and weight by feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6437463"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dd.read_parquet(get_y_fn()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate minimum distances between points (same-fold excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example script provided by GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_distances(block, fold_ids):\n",
    "    unique_fold_ids = np.unique(fold_ids)\n",
    "    min_distances = []\n",
    "\n",
    "    for fold_id in unique_fold_ids:\n",
    "        # Filter out rows with the same fold ID\n",
    "        mask = fold_ids != fold_id\n",
    "        block_filtered = block[mask]\n",
    "        fold_block = block[fold_ids == fold_id]\n",
    "\n",
    "        # Calculate pairwise distances\n",
    "        distances = pairwise_distances(fold_block, block_filtered)\n",
    "\n",
    "        # Get the minimum distance for each row in the fold_block\n",
    "        min_distances.append(np.min(distances, axis=1))\n",
    "\n",
    "    return np.concatenate(min_distances)\n",
    "\n",
    "\n",
    "# Apply the function to each partition\n",
    "dask_array = ddf.drop(columns=\"fold_id\").to_dask_array(lengths=True)\n",
    "fold_ids = ddf[\"fold_id\"].to_dask_array(lengths=True)\n",
    "\n",
    "min_distances = dask_array.map_blocks(calculate_min_distances, fold_ids, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load imputed predict data\n",
    "\n",
    "In order to compare our \"new\" and \"train\" feature spaces, we will need to compute the distance between them. Let's get started by loading our \"new\" data (AKA `predict`).\n",
    "\n",
    "Although our models can tolerate missing values in the data (an important attribute as not all of data variables are available simultaneously at all locations), to calculate pairwise distances or K-d trees, a dense matrix is required (i.e. no missing values).\n",
    "\n",
    "This leaves us with a few options:\n",
    "\n",
    "a) drop all observations in “new” and “train” that contain any missing features (and therefore misrepresent the actual spatial coverage of the training data as well as reduce feature space variance present in DI calculation compared to actual reference data used in training), possibly resulting in an overly pessimistic AoA;\n",
    "\n",
    "b) drop features in both the “new” and “train” data that contain any missing values, likely resulting in an overly optimistic AoA as feature-space complexity is reduced; or \n",
    "\n",
    "c) a “middle ground” approach of imputing missing values and assuming that the resulting predictor space is still a robust representation of the true reference data.\n",
    "\n",
    "To retain as much of the original signature of the true reference data when calculating dissimilarity, as well as to ensure that final AoA maps match the geographic extent of the predictions, we will choose option “c”. It should be noted, however, that, given the novelty of this method, room for improvement likely exists. \n",
    "\n",
    "### Imputation method\n",
    "\n",
    "To achieve the best possible missing value imputation we can use the `NaNImputer` method from the python `verstack` library, which fits gradient boosted tree regression models for each feature to fill missing values (Zherebtsov, 2020/2023). However, since we're working at high resolution with global extent, even small patches of missing data means lots of missing values, which will require some pretty intensive computation time for `NaNImputer` as it utilizes gradient boosting using LightGBM to fit regressors to each feature for imputation (https://verstack.readthedocs.io/en/latest/index.html#nanimputer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ETH_GlobalCanopyHeight_2020_v1</th>\n",
       "      <th>ETH_GlobalCanopyHeightSD_2020_v1</th>\n",
       "      <th>sur_refl_b03_2001-2024_m3_mean</th>\n",
       "      <th>sur_refl_b04_2001-2024_m11_mean</th>\n",
       "      <th>sur_refl_b05_2001-2024_m8_mean</th>\n",
       "      <th>sur_refl_b02_2001-2024_m5_mean</th>\n",
       "      <th>sur_refl_b01_2001-2024_m10_mean</th>\n",
       "      <th>sur_refl_ndvi_2001-2024_m1_mean</th>\n",
       "      <th>sur_refl_b03_2001-2024_m1_mean</th>\n",
       "      <th>sur_refl_b05_2001-2024_m12_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>vodca_k-band_p5</th>\n",
       "      <th>vodca_x-band_mean</th>\n",
       "      <th>wc2.1_30s_bio_15</th>\n",
       "      <th>wc2.1_30s_bio_4</th>\n",
       "      <th>wc2.1_30s_bio_13-14</th>\n",
       "      <th>wc2.1_30s_bio_7</th>\n",
       "      <th>wc2.1_30s_bio_12</th>\n",
       "      <th>wc2.1_30s_bio_1</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8733</td>\n",
       "      <td>3559</td>\n",
       "      <td>3653</td>\n",
       "      <td>3295</td>\n",
       "      <td>2156</td>\n",
       "      <td>-226</td>\n",
       "      <td>2707</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>21161</td>\n",
       "      <td>18950</td>\n",
       "      <td>67.323479</td>\n",
       "      <td>1262.418335</td>\n",
       "      <td>70.0</td>\n",
       "      <td>43.699997</td>\n",
       "      <td>365.0</td>\n",
       "      <td>-4.900000</td>\n",
       "      <td>65.615</td>\n",
       "      <td>-161.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>6549</td>\n",
       "      <td>4598</td>\n",
       "      <td>3370</td>\n",
       "      <td>2333</td>\n",
       "      <td>1561</td>\n",
       "      <td>-215</td>\n",
       "      <td>6088</td>\n",
       "      <td>2841</td>\n",
       "      <td>...</td>\n",
       "      <td>16250</td>\n",
       "      <td>12988</td>\n",
       "      <td>51.977913</td>\n",
       "      <td>1035.702393</td>\n",
       "      <td>73.0</td>\n",
       "      <td>35.800003</td>\n",
       "      <td>497.0</td>\n",
       "      <td>-0.220833</td>\n",
       "      <td>61.115</td>\n",
       "      <td>-160.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1696</td>\n",
       "      <td>1322</td>\n",
       "      <td>2056</td>\n",
       "      <td>1782</td>\n",
       "      <td>686</td>\n",
       "      <td>994</td>\n",
       "      <td>928</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>27081</td>\n",
       "      <td>19413</td>\n",
       "      <td>53.933914</td>\n",
       "      <td>1369.168823</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.700001</td>\n",
       "      <td>330.0</td>\n",
       "      <td>-2.437500</td>\n",
       "      <td>64.925</td>\n",
       "      <td>-147.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>9558</td>\n",
       "      <td>7117</td>\n",
       "      <td>4395</td>\n",
       "      <td>6202</td>\n",
       "      <td>2140</td>\n",
       "      <td>-636</td>\n",
       "      <td>8650</td>\n",
       "      <td>3315</td>\n",
       "      <td>...</td>\n",
       "      <td>12126</td>\n",
       "      <td>9466</td>\n",
       "      <td>59.336483</td>\n",
       "      <td>898.583069</td>\n",
       "      <td>113.0</td>\n",
       "      <td>30.099998</td>\n",
       "      <td>682.0</td>\n",
       "      <td>-1.033333</td>\n",
       "      <td>60.635</td>\n",
       "      <td>-164.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>8091</td>\n",
       "      <td>5478</td>\n",
       "      <td>3082</td>\n",
       "      <td>3082</td>\n",
       "      <td>1331</td>\n",
       "      <td>-587</td>\n",
       "      <td>7383</td>\n",
       "      <td>2045</td>\n",
       "      <td>...</td>\n",
       "      <td>9968</td>\n",
       "      <td>8306</td>\n",
       "      <td>65.759773</td>\n",
       "      <td>1007.976074</td>\n",
       "      <td>98.0</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>539.0</td>\n",
       "      <td>-1.304167</td>\n",
       "      <td>61.935</td>\n",
       "      <td>-164.065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ETH_GlobalCanopyHeight_2020_v1  ETH_GlobalCanopyHeightSD_2020_v1  \\\n",
       "0                               5                                 7   \n",
       "1                               9                                15   \n",
       "2                               8                                10   \n",
       "3                               8                                12   \n",
       "4                              11                                15   \n",
       "\n",
       "   sur_refl_b03_2001-2024_m3_mean  sur_refl_b04_2001-2024_m11_mean  \\\n",
       "0                            8733                             3559   \n",
       "1                            6549                             4598   \n",
       "2                            1696                             1322   \n",
       "3                            9558                             7117   \n",
       "4                            8091                             5478   \n",
       "\n",
       "   sur_refl_b05_2001-2024_m8_mean  sur_refl_b02_2001-2024_m5_mean  \\\n",
       "0                            3653                            3295   \n",
       "1                            3370                            2333   \n",
       "2                            2056                            1782   \n",
       "3                            4395                            6202   \n",
       "4                            3082                            3082   \n",
       "\n",
       "   sur_refl_b01_2001-2024_m10_mean  sur_refl_ndvi_2001-2024_m1_mean  \\\n",
       "0                             2156                             -226   \n",
       "1                             1561                             -215   \n",
       "2                              686                              994   \n",
       "3                             2140                             -636   \n",
       "4                             1331                             -587   \n",
       "\n",
       "   sur_refl_b03_2001-2024_m1_mean  sur_refl_b05_2001-2024_m12_mean  ...  \\\n",
       "0                            2707                               74  ...   \n",
       "1                            6088                             2841  ...   \n",
       "2                             928                               71  ...   \n",
       "3                            8650                             3315  ...   \n",
       "4                            7383                             2045  ...   \n",
       "\n",
       "   vodca_k-band_p5  vodca_x-band_mean  wc2.1_30s_bio_15  wc2.1_30s_bio_4  \\\n",
       "0            21161              18950         67.323479      1262.418335   \n",
       "1            16250              12988         51.977913      1035.702393   \n",
       "2            27081              19413         53.933914      1369.168823   \n",
       "3            12126               9466         59.336483       898.583069   \n",
       "4             9968               8306         65.759773      1007.976074   \n",
       "\n",
       "   wc2.1_30s_bio_13-14  wc2.1_30s_bio_7  wc2.1_30s_bio_12  wc2.1_30s_bio_1  \\\n",
       "0                 70.0        43.699997             365.0        -4.900000   \n",
       "1                 73.0        35.800003             497.0        -0.220833   \n",
       "2                 46.0        47.700001             330.0        -2.437500   \n",
       "3                113.0        30.099998             682.0        -1.033333   \n",
       "4                 98.0        33.500000             539.0        -1.304167   \n",
       "\n",
       "        y        x  \n",
       "0  65.615 -161.725  \n",
       "1  61.115 -160.315  \n",
       "2  64.925 -147.365  \n",
       "3  60.635 -164.635  \n",
       "4  61.935 -164.065  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.dataset_utils import get_predict_imputed_fn\n",
    "\n",
    "\n",
    "pred_imputed = dd.read_parquet(get_predict_imputed_fn())\n",
    "pred_imputed.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traits-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
