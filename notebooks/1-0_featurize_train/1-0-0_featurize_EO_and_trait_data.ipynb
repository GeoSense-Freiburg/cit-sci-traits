{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.0: Featurize EO and trait data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dask import compute, delayed\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import config\n",
    "import pandas as pd\n",
    "import rioxarray as riox\n",
    "from tqdm.notebook import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "from src.conf.conf import get_config\n",
    "from src.conf.environment import log\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(dashboard_address=\":39143\", n_workers=30, memory_limit=\"150GB\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine EO data and one trait as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_map_fns = []\n",
    "\n",
    "for dataset in cfg.datasets.Y.use:\n",
    "    trait_maps_dir = (\n",
    "        Path(cfg.interim_dir)\n",
    "        / cfg[dataset].interim.dir\n",
    "        / cfg[dataset].interim.traits\n",
    "        / cfg.PFT\n",
    "        / cfg.model_res\n",
    "    )\n",
    "    trait_map_fns += list(trait_maps_dir.glob(\"*.tif\"))\n",
    "\n",
    "\n",
    "# Sort trait_map_fns by number in file name (eg. X1, X2, X3)\n",
    "trait_map_fns = sorted(trait_map_fns, key=lambda x: int(x.stem.split(\"X\")[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's get the EO data filenames as well, and then add our trait map filename to the list so that we can load everything simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_data_dir = Path(cfg.interim_dir) / cfg.eo_data.interim.dir / cfg.model_res\n",
    "eo_fns = sorted(list(eo_data_dir.glob(\"*/*.tif\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our preprocessing steps ensured that all of our rasters have the same extent and resolution, we can now load them all into a chunked `xarray.Dataset`, and then convert directly to a `dask.dataframe`. This means we can load 151 global, ~1km resolution rasters (150 EO variables + our single trait map) into a single dataframe in a matter of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "NCHUNKS = 9  # npartitions will be NCHUNKS**2\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_dtype_map(fn) -> Tuple[str, str]:\n",
    "    band = 1\n",
    "    da = riox.open_rasterio(\n",
    "        fn,\n",
    "        chunks={\"x\": 36000 // NCHUNKS, \"y\": 18000 // NCHUNKS},\n",
    "        mask_and_scale=False,\n",
    "    )\n",
    "    long_name = da.attrs[\"long_name\"]\n",
    "\n",
    "    if fn.stem[0] == \"X\":\n",
    "        band = cfg.datasets.Y.trait_stat\n",
    "        long_name = f\"{fn.stem}_{long_name[band - 1]}\"\n",
    "        da.attrs[\"long_name\"] = long_name\n",
    "\n",
    "    dtype = da.sel(band=band).dtype\n",
    "    da.close()\n",
    "\n",
    "    return long_name, str(dtype)\n",
    "\n",
    "\n",
    "@delayed\n",
    "def load_X_raster(fn) -> Tuple[xr.DataArray, str]:\n",
    "    band = 1\n",
    "    da = riox.open_rasterio(\n",
    "        fn,\n",
    "        chunks={\"x\": 36000 // NCHUNKS, \"y\": 18000 // NCHUNKS},\n",
    "        mask_and_scale=True,\n",
    "    )\n",
    "    long_name = da.attrs[\"long_name\"]\n",
    "\n",
    "    return da.sel(band=band), long_name\n",
    "\n",
    "\n",
    "@delayed\n",
    "def load_Y_raster(trait_stem: str) -> Tuple[xr.DataArray, str]:\n",
    "    # find all matching files in fns\n",
    "    trait_fns = [fn for fn in trait_map_fns if fn.stem == trait_stem]\n",
    "\n",
    "    if len(trait_fns) == 0:\n",
    "        raise ValueError(f\"No files found for trait {trait_stem}\")\n",
    "\n",
    "    das = []\n",
    "    for raster_file in trait_fns:\n",
    "        da = riox.open_rasterio(\n",
    "            raster_file,\n",
    "            chunks={\"x\": 36000 // NCHUNKS, \"y\": 18000 // NCHUNKS},\n",
    "            mask_and_scale=True,\n",
    "        )\n",
    "\n",
    "        band = cfg.datasets.Y.trait_stat\n",
    "        long_name = da.attrs[\"long_name\"]\n",
    "        long_name = f\"{raster_file.stem}_{long_name[band - 1]}\"\n",
    "        da.attrs[\"long_name\"] = long_name\n",
    "\n",
    "        das.append(da.sel(band=band))\n",
    "\n",
    "    if len(das) == 1:\n",
    "        return das[0], long_name\n",
    "\n",
    "    else:\n",
    "        # Find the array position of the fn in trait_fns that contains \"gbif\"\n",
    "        gbif_idx = [i for i, fn in enumerate(trait_fns) if \"gbif\" in str(fn)][0]\n",
    "        splot_idx = 1 - gbif_idx\n",
    "\n",
    "        merged = xr.where(\n",
    "            das[splot_idx].notnull(), das[splot_idx], das[gbif_idx], keep_attrs=True\n",
    "        )\n",
    "\n",
    "        for da in das:\n",
    "            da.close()\n",
    "\n",
    "        return merged, long_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when we only load a single trait and handle everything with dask it's still a lot of data to load into memory. To make things more manageable, we are loading the datasets using `dask.delayed`, and then we're computing each partition of the resulting `dask.dataframe` independently before finally concatenating the results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = set(compute(*[get_dtype_map(fn) for fn in eo_fns + trait_map_fns]))\n",
    "X_arrays = list(compute(*[load_X_raster(fn) for fn in eo_fns]))\n",
    "unique_traits = set([fn.stem for fn in trait_map_fns])\n",
    "Y_arrays = list(compute(*[load_Y_raster(trait) for trait in unique_traits]))\n",
    "\n",
    "all_arrays = X_arrays + Y_arrays\n",
    "array_dict = {long_name: da for da, long_name in all_arrays}\n",
    "\n",
    "# Create dtype dictionary for type casting after dropping nans\n",
    "dtypes = {long_name: dtype for long_name, dtype in dtypes}\n",
    "\n",
    "# Create DataArray dict containing the name of each DA and the DA itself.\n",
    "# This will be used \n",
    "combined_ds = xr.Dataset(array_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will collect the names of the EO data arrays (X) and the trait arrays (Y). These will also be the column names in the final dataframe, and we can then drop all NA rows where any X value is missing and/or all Y values are missing. This requires two `.dropna()` calls, as we can't simply drop rows where some, but not all Y values are missing because that would discard valuable training data. On the flipside, we do want to drop rows where we don't have a complete suite of predictors because otherwise calculating the Area of Applicability will fail due to missing values.\n",
    "\n",
    "**Note:* If we choose to use XGBoost as the model, it *can* handle missing values. It may be that we can find a suitable method for handling missing predictor data when calculating the AoA (e.g. with imputation), but for now we are choosing to simply drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the predictor and trait arrays\n",
    "all_names = list(dtypes.keys())\n",
    "X_names = all_names[:len(eo_fns)]\n",
    "Y_names = all_names[len(eo_fns):]\n",
    "\n",
    "with config.set(**{\"array.slicing.split_large_chunks\": False}):\n",
    "    # Convert to Dask DataFrame and drop missing values\n",
    "    ddf = (\n",
    "        combined_ds.to_dask_dataframe()\n",
    "        .drop(columns=[\"band\", \"spatial_ref\"])\n",
    "        .dropna(how=\"all\", subset=Y_names)\n",
    "        .dropna(how=\"any\", subset=X_names)\n",
    "        .astype(dtypes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `dask` task graph now generated, we can begin computing the dataframe by iterating over each partition. If we don't do this, the task graph will eventually overflow our available memory. This is just one of many ways to take advantage of the efficiency and parallelization of `dask`. Once the partitions have been computed, we can simply concatenate the `dfs` with regular `pandas`.\n",
    "\n",
    "This is possible because, since we first loaded all the rasters into the same `xarray.Dataset`, we know that each partition contains entirely unique XY coordinates, and therefore no coordinates could be duplicated across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf11876eae17460fb47a3bb4fb7301d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing partitions:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:22:42 UTC - asyncio - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-259849' coro=<Client._gather.<locals>.wait() done, defined at /home/dl1070/micromamba/envs/traits/lib/python3.12/site-packages/distributed/client.py:2197> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dl1070/micromamba/envs/traits/lib/python3.12/site-packages/distributed/client.py\", line 2206, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute in smaller chunks\u001b[39;00m\n\u001b[1;32m      2\u001b[0m npartitions \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mnpartitions\n\u001b[1;32m      3\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mddf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(npartitions), total\u001b[38;5;241m=\u001b[39mnpartitions, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Concatenate the chunks\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/micromamba/envs/traits/lib/python3.12/site-packages/dask_expr/_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/traits/lib/python3.12/site-packages/dask/base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/micromamba/envs/traits/lib/python3.12/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/micromamba/envs/traits/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/micromamba/envs/traits/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute in smaller chunks\n",
    "npartitions = ddf.npartitions\n",
    "dfs = [\n",
    "    ddf.get_partition(i).compute()\n",
    "    for i in tqdm(range(npartitions), total=npartitions, desc=\"Computing partitions\")\n",
    "]\n",
    "\n",
    "# Concatenate the chunks\n",
    "df = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>ETH_GlobalCanopyHeightSD_2020_v1</th>\n",
       "      <th>ETH_GlobalCanopyHeight_2020_v1</th>\n",
       "      <th>sur_refl_b01_2001-2024_m10_mean</th>\n",
       "      <th>sur_refl_b01_2001-2024_m11_mean</th>\n",
       "      <th>sur_refl_b01_2001-2024_m12_mean</th>\n",
       "      <th>sur_refl_b01_2001-2024_m1_mean</th>\n",
       "      <th>sur_refl_b01_2001-2024_m2_mean</th>\n",
       "      <th>sur_refl_b01_2001-2024_m3_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>vodca_x-band_mean</th>\n",
       "      <th>vodca_x-band_p5</th>\n",
       "      <th>vodca_x-band_p95</th>\n",
       "      <th>wc2.1_30s_bio_1</th>\n",
       "      <th>wc2.1_30s_bio_12</th>\n",
       "      <th>wc2.1_30s_bio_13-14</th>\n",
       "      <th>wc2.1_30s_bio_15</th>\n",
       "      <th>wc2.1_30s_bio_4</th>\n",
       "      <th>wc2.1_30s_bio_7</th>\n",
       "      <th>X50_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-179.895</td>\n",
       "      <td>68.395</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10000</td>\n",
       "      <td>1675</td>\n",
       "      <td>41</td>\n",
       "      <td>259</td>\n",
       "      <td>7960</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>12914</td>\n",
       "      <td>12956</td>\n",
       "      <td>12755</td>\n",
       "      <td>-12.245833</td>\n",
       "      <td>365.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.364677</td>\n",
       "      <td>1201.524536</td>\n",
       "      <td>37.800003</td>\n",
       "      <td>1.447954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-179.895</td>\n",
       "      <td>68.255</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6161</td>\n",
       "      <td>1449</td>\n",
       "      <td>44</td>\n",
       "      <td>197</td>\n",
       "      <td>6213</td>\n",
       "      <td>7977</td>\n",
       "      <td>...</td>\n",
       "      <td>13841</td>\n",
       "      <td>13844</td>\n",
       "      <td>13457</td>\n",
       "      <td>-11.654166</td>\n",
       "      <td>340.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>51.789902</td>\n",
       "      <td>1237.107056</td>\n",
       "      <td>39.200001</td>\n",
       "      <td>1.063078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-179.885</td>\n",
       "      <td>67.365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4586</td>\n",
       "      <td>1753</td>\n",
       "      <td>68</td>\n",
       "      <td>485</td>\n",
       "      <td>5751</td>\n",
       "      <td>7228</td>\n",
       "      <td>...</td>\n",
       "      <td>15345</td>\n",
       "      <td>14444</td>\n",
       "      <td>15253</td>\n",
       "      <td>-11.316667</td>\n",
       "      <td>384.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.370106</td>\n",
       "      <td>1263.077637</td>\n",
       "      <td>40.199997</td>\n",
       "      <td>1.369326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-179.875</td>\n",
       "      <td>68.705</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4641</td>\n",
       "      <td>2243</td>\n",
       "      <td>30</td>\n",
       "      <td>138</td>\n",
       "      <td>6857</td>\n",
       "      <td>8364</td>\n",
       "      <td>...</td>\n",
       "      <td>9674</td>\n",
       "      <td>9398</td>\n",
       "      <td>10504</td>\n",
       "      <td>-11.075000</td>\n",
       "      <td>291.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>48.830692</td>\n",
       "      <td>1281.555298</td>\n",
       "      <td>40.299999</td>\n",
       "      <td>1.630068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-179.855</td>\n",
       "      <td>68.735</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4848</td>\n",
       "      <td>2138</td>\n",
       "      <td>27</td>\n",
       "      <td>124</td>\n",
       "      <td>6260</td>\n",
       "      <td>7932</td>\n",
       "      <td>...</td>\n",
       "      <td>9609</td>\n",
       "      <td>9321</td>\n",
       "      <td>10456</td>\n",
       "      <td>-11.366667</td>\n",
       "      <td>300.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.124699</td>\n",
       "      <td>1257.740234</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>2.958496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x       y  ETH_GlobalCanopyHeightSD_2020_v1  \\\n",
       "0 -179.895  68.395                                 8   \n",
       "1 -179.895  68.255                                 4   \n",
       "2 -179.885  67.365                                 1   \n",
       "3 -179.875  68.705                                 2   \n",
       "4 -179.855  68.735                                 1   \n",
       "\n",
       "   ETH_GlobalCanopyHeight_2020_v1  sur_refl_b01_2001-2024_m10_mean  \\\n",
       "0                               4                            10000   \n",
       "1                               3                             6161   \n",
       "2                               0                             4586   \n",
       "3                               1                             4641   \n",
       "4                               0                             4848   \n",
       "\n",
       "   sur_refl_b01_2001-2024_m11_mean  sur_refl_b01_2001-2024_m12_mean  \\\n",
       "0                             1675                               41   \n",
       "1                             1449                               44   \n",
       "2                             1753                               68   \n",
       "3                             2243                               30   \n",
       "4                             2138                               27   \n",
       "\n",
       "   sur_refl_b01_2001-2024_m1_mean  sur_refl_b01_2001-2024_m2_mean  \\\n",
       "0                             259                            7960   \n",
       "1                             197                            6213   \n",
       "2                             485                            5751   \n",
       "3                             138                            6857   \n",
       "4                             124                            6260   \n",
       "\n",
       "   sur_refl_b01_2001-2024_m3_mean  ...  vodca_x-band_mean  vodca_x-band_p5  \\\n",
       "0                           10000  ...              12914            12956   \n",
       "1                            7977  ...              13841            13844   \n",
       "2                            7228  ...              15345            14444   \n",
       "3                            8364  ...               9674             9398   \n",
       "4                            7932  ...               9609             9321   \n",
       "\n",
       "   vodca_x-band_p95  wc2.1_30s_bio_1  wc2.1_30s_bio_12  wc2.1_30s_bio_13-14  \\\n",
       "0             12755       -12.245833             365.0                 42.0   \n",
       "1             13457       -11.654166             340.0                 42.0   \n",
       "2             15253       -11.316667             384.0                 46.0   \n",
       "3             10504       -11.075000             291.0                 36.0   \n",
       "4             10456       -11.366667             300.0                 35.0   \n",
       "\n",
       "   wc2.1_30s_bio_15  wc2.1_30s_bio_4  wc2.1_30s_bio_7  X50_mean  \n",
       "0         44.364677      1201.524536        37.800003  1.447954  \n",
       "1         51.789902      1237.107056        39.200001  1.063078  \n",
       "2         47.370106      1263.077637        40.199997  1.369326  \n",
       "3         48.830692      1281.555298        40.299999  1.630068  \n",
       "4         46.124699      1257.740234        39.500000  2.958496  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's check to see how the size of our final features compares to the original size of our GBIF trait maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X95 has 5,944,102 trait values and 4,827,760 feature values (81.219333%%)\n"
     ]
    }
   ],
   "source": [
    "gbif_trait_map_fns = (\n",
    "    Path(cfg.interim_dir)\n",
    "    / cfg.gbif.interim.dir\n",
    "    / cfg.gbif.interim.traits\n",
    "    / cfg.PFT\n",
    "    / cfg.model_res\n",
    ").glob(\"*.tif\")\n",
    "\n",
    "feats = pd.read_parquet(\n",
    "    Path(cfg.train.dir)\n",
    "    / cfg.PFT\n",
    "    / cfg.model_res\n",
    "    / \"_\".join(cfg.datasets.Y.use)\n",
    "    / cfg.train.features\n",
    ")\n",
    "\n",
    "for fn in list(gbif_trait_map_fns)[:1]:\n",
    "    trait_count = riox.open_rasterio(fn).sel(band=1).notnull().sum().values.item()\n",
    "    feat_count = feats[[f\"{fn.stem}_mean\"]].dropna().shape[0]\n",
    "    print(\n",
    "        f\"{fn.stem} has {trait_count:,} trait values and {feat_count:,} feature values ({feat_count / trait_count:%}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to match about 81% of our original GBIF trait values with EO predictors! This may seem surprising, since we would expect our predictors to have at least as broad of coverage as our GBIF observations, but in fact many GBIF observations have locations that are in water and built-up areas, both of which were masked when harmonizing the EO data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
